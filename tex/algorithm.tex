%%%%%%%%%%%%%%%%%%          gtlatex.tem       %%%%%%%%%%%%%%%%%%
%
%   Template for articles written in LaTeX for publication in
%   G&T, G&TM and A&GT.  This template must be used with latex2e.  
%   If you use BiBTeX then you can collect the bibliography style 
%   file  gtart.bst  from
%       https://msp.org/gtp/macros/gtart.bst
%   instructions for using gtpart.cls are given in gtpartdoc.pdf,
%   available at
%       https://msp.org/gtp/macros/gtpartdoc.pdf
%
\documentclass[microtype]{gtpart}     % Basic GT/GTM/AGT style
%
%   The microtype option considerably improves document layout and
%   will make your article more closely approximate the final
%   published version. (This option requires the use of pdflatex;
%   if you use latex instead, you can just remove the option.)
%
%   Uncomment one of the next three lines to obtain a full "mock-up"
%   of a published article:
%   A&GT:  \agtart     G&T:  \gtart   G&TM:  \gtmonart
%
%   NOTE:  Please do not place your article in a public place (eg
%          on the arXiv) in "mock-up" form unless it has been accepted
%          for publication in the relevant journal.
%
%\gtart  
%\agtart
%\gtmonart
%
%   Add necessary packages here.  Note that amsthm, amssymb and
%   amsmath are already loaded, so there is no need to add any 
%   of these.  Examples:
%
\usepackage{pinlabel}  %%% the recommended graphics+labelling package
\usepackage{graphicx}  %%% the recommended graphics package
\usepackage[all]{xy}
\usepackage{amscd}
\usepackage{cleveref}


%%% Start of metadata

\title{Algorithm Details}

%  First author
%
\author{}
\givenname{}
\surname{}
\address{}
\email{}
\urladdr{}

%  Second author (uncomment if necessary)
%
%\author{}
%\givenname{}
%\surname{}
%\address{}
%\email{}
%\urladdr{}
%
%  (Add a similar block for other authors)
%
%   Title and author both have running head options:
%
%   \title[Running head title]{Main title}
%   \author[Running head author]{Author}
%
% give a separate \keyword and \subject line for each keyword/phrase or 
% subject class eg \keyword{framed link} \subject{primary}{msc2010}{57M25}

%\keyword{}
%\subject{primary}{msc2010}{}
%\subject{secondary}{msc2010}{}

%  Fill in the reference number if your article is stored on the arXiv
%  eg \arxivreference{math.GT/0512347} or \arxivreference{1203.4984}.
%  The newer style reference numbers (with a period) do not require the
%  prefix arxiv: or math.NT/ or anything else. Just the reference
%  number is sufficient.

\arxivreference{}

%%% End of metadata

%%% Start of user-defined macros %%%
%
%   Theorem-type environments.  There are two predefined styles :
%
%   \theoremstyle{plain} : for theorems, corollaries etc with heading 
%   bold and left justified, optional note bracketed in roman type
%   and statement in slanted type.  This is the default style.
%
%   \theoremstyle{definition} : (alias remark)  for definitions, remarks 
%   etc with heading bold and left justified, optional note as before but
%   with statement in roman type.
%   
%   Some sample  \newtheorem's  (delete these unless you need
%   them and insert your own):
%
\newtheorem{thm}{Theorem}[section]    % Standard theorem environment
\newtheorem{lem}[thm]{Lemma}          % Lemma environment with numbering 
\newtheorem{prop}[thm]{Proposition}   % Proposition environment with numbering
%                                     % consecutive to theorems
\newtheorem*{zlem}{Zorn's Lemma}      % A special unnumbered lemma.
%
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}    % Definition environment with 
%                                     % numbering consecutive to theorems
\newtheorem*{rem}{Remark}             % Unnumbered environment for remarks.
%
%   Type your macros (\newcommand's etc) below.
%
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


%%% End of user-defined macros %%%

\begin{document}

\begin{abstract}    % type your abstract below

In this article, we describe the algorithm in this project.

\end{abstract}

\maketitle

%%%%%%%%%%%%%%%%%%%%   Start of main body of article

\section{Introduction}

Naive implementations of Kalman filters and smoothers have numerical instability issues.
In this project, the square root formulation of the Kalman algorithm is implemented, which is more accurate and stable. 

In case the dynamics are presented by multiple models,
an approach to the multiple model fixed interval smoothing problem for the square root Kalman algorithms is examined.
The approach is based on the interacting multiple models (IMM), except that we consider the square root Kalman algorithms.

\section{Square Root Kalman Algorithm}

In this section, we describe the square root Kalman algorithm described in \cite{bierman2006factorization}.

Let $X$ be a Gaussian random variable with mean $m$ and covariance matrix $V$.
The square root form of $X$ is given by $R$ and $z$ where
\begin{align*}
 (R^TR)^{-1} &= V, \\
 Rm &= z.
\end{align*}
The square root formulation of the Kalman algorithms:
\begin{itemize}
	\item square root information filter and
	\item square root information smoother
\end{itemize}
will be parametrized by Gaussian random variables in square root form.

Recall that in $k$th iteration, the true state $x_k$ is evolved from the state $x_{k-1}$ according to
\begin{align}
x_k = \Phi_k x_{k-1} + Gw_k \label{eq:predict}
\end{align}
where
\begin{itemize}
	\item $\Phi_k$ is the nonsingular transition matrix and
	\item $w_k$ is the process noise that are assumed to have zero mean and nonsingular covariance matrix 
	$Q_w(k)=(R^T_w(k)R_w(k))^{-1}$.
\end{itemize}
An observation $z_k$ of the true state $x_k$ is made according to
\begin{align*}
	&z_k = A x_k + v_k & &\text{for } 1\leq k \leq T
\end{align*}
where $v_k \in N(0, I)$.

\subsection{Square Root Information Filter}

A filtering problem is to solve
\begin{align*}
& \underset{x_T}{\text{minimize}} & & \norm{\tilde{R}_x(1)x_1 - \tilde{z}_x(1)} + \sum_{k=1}^{T}\norm{Ax_k - z_k}^2 \\
& \text{subject to} & & \cref{eq:predict}.
\end{align*}

\subsubsection{Prediction Step}

Let $(\hat{R}_x(k-1), \hat{z}_x(k-1))$ be the square root form of the updated estimate in iteration $k-1$.
The prediction step is to solve
\begin{align*}
	& \underset{x_k}{\text{minimize}} & &\norm{\hat{R}_x(k-1)x_{k-1} - \hat{z}_x(k-1)}^2 + \norm{R_w(k)w_j - z_w(k)}^2 \\
	& \text{subject to} & & \cref{eq:predict}.
\end{align*}

Noted that $x_{k-1} = \Phi^{-1}_k(x_k - Gw_k)$, the above minimization problem is equivalent to
\begin{align*}
	& \underset{x_k, w_k}{\text{minimize}} & &\norm{
		\begin{pmatrix}
		R_w(k) &0 \\
		-R^d_x(k)G &R^d_x(k)
		\end{pmatrix}
		\begin{pmatrix}
		w_k \\ x_k
		\end{pmatrix} - \begin{pmatrix}
		z_w(k) \\ \hat{z}_x(k-1)
		\end{pmatrix}}^2
\end{align*}
which can be solved by using the QR decomposition
\begin{align*}
	\begin{pmatrix}
		R_w(k) &0 &z_w(k) \\ 
		-R^d_x(k)G &R^d_x(k) &\hat{z}_x(k-1)
	\end{pmatrix} = Q
	\begin{pmatrix}
		\tilde{R}_w(k) &\tilde{R}_{wx}(k) &\tilde{z}_w(k) \\ 0 &\tilde{R}_x(k) &\tilde{z}_x(k)
	\end{pmatrix}
\end{align*}
where
\begin{itemize}
	\item the right hand side is the QR decomposition of the left hand side,
	\item $(R_w(k), z_w(k))$ is the prior estimate of $w_k$ in square root form,
	\item $R^d_x(k)=\hat{R}_x(k-1)\Phi^{-1}_k$ and
	\item $(\tilde{R}_x(k), \tilde{z}_x(k))$ is the predict estimate in square root form.
\end{itemize}

\subsubsection{Update Step}

Let $(\tilde{R}_x(k), \tilde{z}_x(k))$ be the prior information obtained from prediction step, 
the update step is to solve
\begin{align*}
	& \underset{x_k}{\text{minimize}} & &\norm{\tilde{R}_x(k)x_k - \tilde{z}_x(k)}^2 + \norm{Ax_k - z_k}^2.
\end{align*}
The above minimization problem can be rewritten as
\begin{align*}
	& \underset{x_k}{\text{minimize}} & &\norm{
	\begin{pmatrix}
	\tilde{R}_x(k) \\ \hat{R}_x(k)
	\end{pmatrix}x_k - 
	\begin{pmatrix}
	\tilde{z}_x(k) \\  z_k
	\end{pmatrix}}^2
\end{align*}
which can be solved by using the QR decomposition
\begin{align*}
	\begin{pmatrix}
		\tilde{R}_x(k) &\tilde{z}_x(k) \\ 
		A &z_k
	\end{pmatrix} = Q
	\begin{pmatrix}
		\hat{R}_x(k) &\hat{z}_x(k) \\
		0 &e_k
	\end{pmatrix}
\end{align*}
where
\begin{itemize}
	\item the right hand side is the QR decomposition of the left hand side and
	\item $(\hat{R}_x(k), \hat{z}_x(k))$ is the updated estimate in square root form.
\end{itemize}

\subsection{Square Root Information Smoother}

A smoothing problem is to solve
\begin{align*}
& \underset{x_k}{\text{minimize}} & & \norm{\tilde{R}_x(1)x_1 - \tilde{z}_x(1)} + \sum_{l=1}^{K}\norm{Ax_l - z_l}^2 \\
& \text{subject to} & & \cref{eq:predict}.
\end{align*}
for $1\leq k \leq T$.

\subsubsection{Smooth Step}

Let $(R^*_x(k), z^*_x(k)$ be the smoothed estimate in iteration $k$.
The smooth step is to solve
\begin{align*}
	& \underset{x_{k-1}}{\text{minimize}} & &\norm{\tilde{R}_w(k)w_k + \tilde{R}_{wx}(k)x_k - \tilde{z}_w(k)}^2 +
	\norm{\tilde{R}_x(k)x_k - \tilde{z}_x(k)}^2 \\
	& \text{subject to} & & \cref{eq:predict}.
\end{align*}
Replacing $x_k$ by $\Phi_k x_{k-1} + Gw_k$, the above minimization problem is equivalent to
\begin{align*}
	& \underset{x_{k-1}, w_k}{\text{minimize}} & &\norm{
		\begin{pmatrix}
			\tilde{R}_w(k) + \tilde{R}_{wx}(k)G &\tilde{R}_{wx}(k)\Phi_k \\
			R^*_x(k)G &R^*_x(k)\Phi_k
		\end{pmatrix}
		\begin{pmatrix}
		w_k \\ x_{k-1}
		\end{pmatrix} - 
		\begin{pmatrix}
			\tilde{z}_w(k) \\ z^*_x(k)
		\end{pmatrix}}^2
\end{align*}
which can be solved by using the QR decomposition
\begin{align*}
	\begin{pmatrix}
		\tilde{R}_w(k) + \tilde{R}_{wx}(k)G &\tilde{R}_{wx}(k)\Phi_j &\tilde{z}_w(k) \\
		R^*_x(k)G &R^*_x(k)\Phi_j &z^*_x(k)
	\end{pmatrix} = Q
	\begin{pmatrix}
		R^*_w(k) &R^*_{wx}(k) &z^*_w(k) \\
		0 &R^*_x(k-1) &z^*_x(k-1)
	\end{pmatrix}
\end{align*}
where
\begin{itemize}
	\item the right hand side is the QR decomposition of the left hand side and
	\item $(R^*_x(k-1), z^*_x(k-1))$ is the smoothed estimate in square root form.
\end{itemize}

\section{Square Root IMM Algorithm}

Let $M_k$ be the model in effect during the sampling period ending at time $k$.
The event that model $j$ $(j=1,\ldots,n)$ is in effect during the sampling period ending at time $k$ will be denoted by $M^j_k$.

\subsection{Square Root IMM Filter}

When applying the classical IMM filtering, attentions are needed for the followings:
\begin{itemize}
	\item mixing the state estimates and
	\item model likelihood computation.
\end{itemize}

\subsubsection{Mixing The State Estimates}

\begin{prop}
	Let $a$, $b$ and $c$ be multivariate Gaussian distribution with square root form $(R_a, z_a)$, $(R_b, z_b)$ and $(R_c, z_c)$ respectively.
	Suppose that $c = a + Ab$ for some matrix $A$. Then
	\begin{align}
		\begin{pmatrix}
			R_b &0 &z_b \\
			-R_aA &R_a &z_a
		\end{pmatrix} = \begin{pmatrix}
		-, Q
		\end{pmatrix}\begin{pmatrix}
			H &L &-\\
			0 &R_c &z_c
		\end{pmatrix}
		\label{prop_1:eq_1}
	\end{align}
	the right hand side is the QR decomposition of the left hand side.
	Moreover,
	\begin{align}
		&Q^T\begin{pmatrix}
			0 \\ z_a
		\end{pmatrix} = R_c m_a,
		&&Q^T\begin{pmatrix}
		z_b \\ 0
		\end{pmatrix} = R_c A m_b,
		\label{prop_1:eq_7}
	\end{align}
	where $m_a, m_b$ and $m_c$ are mean of $a, b$ and $c$ respectively
	\label{prop_1}
\end{prop}

\begin{proof}
	Let $V_a, V_b$ and $V_c$ be the covariance matrix of $a, b$ and $c$ respectively.
	We first show that $V_c = V_a + A V_b A^T$.
	Comparing the left and right side of \cref{prop_1:eq_1}, we have
	\begin{align}
		H^TH &= R^T_b R_b + A^T R^T_a R_a A, \label{prop_1:eq_2}\\
		H^TL &= -A^T R^T_a R_a, \label{prop_1:eq_3}\\
		R^T_a R_a &= L^T L + R^T_c R_c. \label{prop_1:eq_4}
	\end{align}
	\cref{prop_1:eq_3} implies that
	\begin{align}
		L = - H^{-T} A^T R^T_a R_a. \label{prop_1:eq_5}
	\end{align}
	
	From \cref{prop_1:eq_2,prop_1:eq_4,prop_1:eq_5}, we have
	\begin{align*}
		R^T_c R_c 	&= R^T_a R_a - L^T L \\
					&= R^T_a R_a - R^T_a R_a A (H^TH)^{-1} A^T R^T_a R_a \\
					&= R^T_a R_a - R^T_a R_a A \left( R^T_b R_b + A^T R^T_a R_a A \right)^{-1} A^T R^T_a R_a \\
					&= V_a^{-1} - V_a^{-1} A \left( V_b^{-1} + A^T V_a^{-1} A \right)^{-1} A^T V_a^{-1} \\
					&= \left( V_a + A V_b A^T \right)^{-1}, \numberthis\label{prop_1:eq_6}
	\end{align*}
	where \cref{prop_1:eq_6} is due to \textit{Woodbury matrix identity}.
	Hence we have $V_c = V_a + A V_b A^T$.
	
	Next, we show that $m_c = m_a + A m_b$. TODO
	
	To show \cref{prop_1:eq_7}, by pre--multiplying $Q^T$ to the left and right side of \cref{prop_1:eq_1}, we obtain that
	\begin{align*}
	content...
	\end{align*}
	
\end{proof}



\subsubsection{Model Likelihood Computation}

%%%%%%%%%%%%%%%%%%%%   End of main body of article
%
%                             References
%
%   BiBTeX users uncomment the following line:
%
\bibliographystyle{plain}
\bibliography{cdata}
%

%\begin{thebibliography}

%\end{thebibliography}

\end{document}

